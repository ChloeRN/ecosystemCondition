---
title: "Climate indicators"
output:
  html_document
---

# Climate indicators

<br />

*Author and date:* <!-- Write your name here -->

<br />

<!-- Load all you dependencies here -->

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
#library(tidyverse)
library(dplyr)
library(ggplot2)
library(stars)
library(sf)
library(tmap)
library(lubridate)
library(ggpubr)
library(microbenchmark)
library(tictoc)
library(ncmeta)
library(parallel)
library(readr)
dir <- substr(getwd(), 1,2)

```

```{r, echo=F}
Ecosystem <- "All" # e.g. "Skog og fjell"
Egenskap  <- "Abiotiske egenskaper" # e.g. "Primærproduksjon"
ECT       <- "Physical state characteristics" # e.g. "Structural state characteristic"
Contact   <- "Anders L. Kolstad" # e.g. "Anders Kolstad"
```

```{r, echo=F}
metaData <- data.frame(Ecosystem,
                       "Økologisk egenskap" = Egenskap,
                       "ECT class" = ECT)
knitr::kable(metaData)
```

<br /> <br />

<hr />

## Introduction

This chapters describes a workflow for generating or preparing indicators based on interpolated climate data from [SeNorge](https://senorge.no/).

## About the underlying data

The data is in a raster format and extends back to 1957 in the form of multiple interpolated climate variables. The spatial resolution is 1 x 1 km.

### Representativity in time and space

The data includes the last normal period (1961-1990) which defines the reference condition for climate variables. Therefore the temporal resolution is very good. Also considering the daily resolution of the data.

Spatially, a 1x1 km resolution is sufficient for most climate variables, esp. in homogeneous terrain, but this needs to be evaluation for each variable and scenario specifically.

### Original units

Varied. Specified below for each parameter.

### Temporal coverage

1957 - present

### Aditional comments about the dataset

The data format has changes from .BIL to .nc (netcdf) and now a single file contains all the rasters for one year (365 days), and sometimes for multiple variables also.

## Ecosystem characteristic

### Norwegain standard

These variables typically will fall under the *abiotiske egenskaper* class.

### SEEA EA

In SEEA EA, these variables will typically fall under A1 - Physical state characteristics.

## Collinearities with other indicators

Climate variables are most likely to be correlated with each other (e.g. temperature and snow). Also, some climate variables are better classed as pressure indicators, and these might have a causal association with several condition indicators.

## Reference condition and values

### Reference condition

The reference condition for climate variables is defined as the laste normal period 1961-1990.

### Reference values, thresholds for defining *good ecological condition*, minimum and/or maximum values

-   Un-scaled indicator value = median value over last 5 year (moving window)

-   Upper reference level (best possible condition) = median value from the reference period

-   Threshold for good ecosystem condition = 2 standard deviation units for the climate variable during the reference period.

-   Lower reference value = 5 standard deviation units for the climate variable during the reference period.

## Uncertainties

For the indicator map (1 x 1 km raster) there is no uncertainty associated with the indicator values. For aggregated indicator values (e.g. for regions), the uncertainty in the indicator value is calculated from the spatial variation in the indicator values via bootstrapping.

## References

<https://senorge.no/>

rr and tm are being download from <https://thredds.met.no/thredds/catalog/senorge/seNorge_2018/Archive/catalog.html>

### Additional resources

[Stars package](https://r-spatial.github.io/stars/)

[R as a GIS for economists](https://tmieno2.github.io/R-as-GIS-for-Economists/stars-basics.html) chapter 7

## --------------------------------------------------

## Analyses

### Data set

The data is downloaded to a local NINA server, and updated regularly.

```{r}
path <- ifelse(dir == "C:", 
      "R:/GeoSpatialData/Meteorology/Norway_SeNorge2018_v22.09/Original",
      "/data/R/GeoSpatialData/Meteorology/Norway_SeNorge2018_v22.09/Original")
```

This folder contains folder for the different parameters

```{r}
(files <- list.files(path))
```

This table explains them in more detail

```{r}
senorgelayers <- read_delim("data/senorgelayers.txt", 
    delim = "\t", escape_double = FALSE, 
    trim_ws = TRUE)
DT::datatable(senorgelayers)
```

Here is the content of one of these folders (first 10 entries)

```{r}
(files_tm <- list.files(paste0(path, "/rr_tm")))[1:10]
```

There are `r length(files_tm)` files, i.e. `r length(files_tm)` years of data, one file per year.

Importing (proxy) file:

```{r, warning=F}
tg_1957 <- stars::read_ncdf(paste0(path, "/rr_tm/seNorge2018_1957.nc"),
                            var="tg")
```

I think I know the CRS, so setting it manually.

```{r}
st_crs(tg_1957) <- 25833
```

The data has three dimension

```{r}
dim(tg_1957)
```

Initially the data had four attributes, but I subsettet to only include _tg_.

```{r}
names(tg_1957)
```

`tg_1957` is a stars proxy object and [most commands](https://r-spatial.github.io/stars/articles/stars8.html) will not initiate any change untill later, typically I write to file and all the lazy operations are done in squeal. Therefore, I will prepare a test data set, which is smaller and which I can import to memory. Then I can perform the operations on that data set and we can see the results.

### Dummy data

This test data is included in the {stars} package

```{r}
tif = system.file("tif/L7_ETMs.tif", package = "stars")
t1 = as.Date("1970-05-31")
x = read_stars(c(tif, tif, tif, tif), along = 
                  list(time = c(t1, t1+1, t1+50, t1+100)), 
               RasterIO = list(nXOff = c(1), 
                               nYOff = c(1), 
                               nXSize = 50, 
                               nYSize = 50, 
                               bands = c(1:6)))
```

A single attribute

```{r}
names(x)
```

I can rename it like this

```{r}
x <- setNames(x, "Attribute_A")
```

And I can add another dummy attribute.

```{r}
x <- x %>%
  mutate("Attribute_B" = Attribute_A/2)
```

The dummy data also has four dimensions

```{r}
dim(x)
```

X and y area the coordinates. Band is an integer:

```{r}
st_get_dimension_values(x, "band")
```

I will remove the *band* dimension.

```{r}
x <- x %>% filter(band==1) %>% adrop()
```

Time is four dates covering four months in 1970:

```{r}
(oldTime <- st_get_dimension_values(x, "time"))
```

I'll add some extra seasonal variation between to the mix

```{r}
x$Attribute_A[,,2] <- x$Attribute_A[,,1]*1.2
x$Attribute_A[,,3] <- x$Attribute_A[,,1]*1.4
x$Attribute_A[,,4] <- x$Attribute_A[,,1]*1.1
```

Now I create another four copies of this data, adding some random noise and a continuous decreasing trend.

```{r}
# Function to add random noise
myRandom <- function(x) x*rnorm(1,1,.05)
```

```{r}
y1 <- x
y2 <- x %>% st_apply(1:3, myRandom) %>% st_set_dimensions("time", values = oldTime %m+% years(-1)) %>% mutate(Attribute_A = Attribute_A*0.95)
y3 <- x %>% st_apply(1:3, myRandom) %>% st_set_dimensions("time", values = oldTime %m+% years(-2)) %>% mutate(Attribute_A = Attribute_A*0.90)
y4 <- x %>% st_apply(1:3, myRandom) %>% st_set_dimensions("time", values = oldTime %m+% years(-3)) %>% mutate(Attribute_A = Attribute_A*0.85)
y5 <- x %>% st_apply(1:3, myRandom) %>% st_set_dimensions("time", values = oldTime %m+% years(-4)) %>% mutate(Attribute_A = Attribute_A*0.80)

```

```{r}
# We combine the data into one cube for plotting:
temp <- c(y1, y2, y3, y4, y5) 

ggplot() + 
  geom_stars(data = temp) +
  coord_equal() +
  theme_void() +
  scale_fill_viridis_c(option = "D") +  
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0))+
  facet_wrap(~time, ncol=4)
```

Saving these to file to demonstrate the importing step of the workflow.

```{r, eval=F}
path <- "P:/41201785_okologisk_tilstand_2022_2023/data/climate_indicators/dummy_data"
saveRDS(y1, paste0(path, "/y1.rds"))
saveRDS(y2, paste0(path, "/y2.rds"))
saveRDS(y3, paste0(path, "/y3.rds"))
saveRDS(y4, paste0(path, "/y4.rds"))
saveRDS(y5, paste0(path, "/y5.rds"))
```

##### Dummy regions and ET map

Here is some dummy data for accounting areas (regions) and ecosystem occurrences as well.

```{r, fig.cap="Showing the example account area delineation (raster format)"}
dummy_aa <- x %>%
  filter(time == oldTime[1]) %>%
  adrop() %>%
  mutate(accuntingAreas = rep(c(1,2), each=length(Attribute_A)/2)) %>%
  select(accuntingAreas)

tm_shape(dummy_aa) +
  tm_raster(style="cat")
```

```{r, fig.cap="Showing the example Ecosystem type delineation data."}
dummy_et <- x %>%
  filter(time == oldTime[1]) %>%
  adrop() %>%
  mutate(ecosystemType = rep(c(1,NA), length.out = length(Attribute_A))) %>%
  select(ecosystemType)

tm_shape(dummy_et) +
  tm_raster(style="cat")
```

#### Regions

Importing a shape file with the regional delineation.

```{r}
reg <- sf::st_read("data/regions.shp")
#st_crs(reg)
```

#### Ecosystem map

Coming soon ....


## --------------------------------------------------

### Conseptual workflow

The general, conceptual workflow is like this:

1.  Collate variable data series

    -   Import .nc files (loop though year 1961-1990) and subset to the correct attribute (above)

    -   Filter data by dates (optional) (`dplyr::filter`)

    -   Aggregate across time within a year (`stars::st_apply`)

    -   Join all data into one data cube (`stars:c`)

2.  Calculate reference values

    -   Aggregate across reference years to get median and sd values (`st_apply)`

    -   Join with existing data cube (`stars:c`)

3.  Calculate indicator values

    -   Normalize climate variable at the individual grid cell level using the three reference values (`mutate(across()))`

4.  Write to file

5.  Mask by ecosystem type (this could also be done in step one to save time)

6.  Aggregate in space (to accounting areas)

    -   Aggregate across 5 year time steps to smooth out random inter-annual variation and leave climate signal

    -   Intersect with accounting area polygons `sf:st_intersects`

    -   Bootstrap indicator values (cell values) and get median and sd

7.  Make trend figure

I will try to use [parallelization](https://tmieno2.github.io/R-as-GIS-for-Economists/EE.html) to speed things up. Also, masking to ecosystem time earlier in the workflow might prove smart.

### 1. Collate variable data series

I include a for loop for importing the .nc files at the end.

#### Filter data by dates

Say we want to calculate the mean summer temperature. We then want to exclude the data for the times that is not withing our definition of summer.

Let's try it on the dummy data. Remember this data had four time steps.

```{r}
st_get_dimension_values(x, "time")
```

Our real data has 365 time steps for each year.

First I define my start and end dates. I want to keep only the summer data, defined as jun - aug.

```{r}
start_month <-  "06"
end_month <- "08"
```

Then for each iteration I need to get the year as well

```{r}
(year_temp <- year(st_get_dimension_values(x, "time")[1]))
```

Then I can create a time interval object

```{r}
start <- ym(paste(year_temp, start_month, sep="-"))
end <-  ym(paste(year_temp, end_month, sep="-"))
(myInterval <- interval(start, end))
```

Then I filter

```{r}
x_aug <- x %>%
  select(Attribute_A) %>%
  adrop() %>%
  filter(time %within% myInterval)
st_get_dimension_values(x_aug, "time")
```

#### Aggregate across time within a year

For this example I want to calculate the mean temperature over the summer. I therefore need to aggregate over time. There are two ways, either using `st_apply` or using `aggregate`.

```{r}
temp_names <- year(st_get_dimension_values(x_aug, "time")[1])

microOut <- microbenchmark(
st_apply =  x_aug %>%
  st_apply(1:2, mean) %>%
  setNames(paste0("v_",temp_names)),
aggregate = x_aug %>%
  aggregate(by = "year", mean),
times=30
)
```

The time dimension is now gone as we have aggregated across it.

```{r, fig.cap="Comparing computation tome for two spatial aggregation functions."}
autoplot(microOut)
```

`st_apply` is slightly faster, but `aggregate` has the advantage of that it retains the time dimension, whereas for `st_apply` I need to set the attribute name to be the year. I will try to use `st_apply`.

But let me see if I can save time by masking to ET before doing this operation.

```{r, fig.cap="Demonstrating the effect of maskig the dummy data using a perfectly aligne raster ET map."}
x_masked <- x_aug
x_masked[is.na(dummy_et)] <- NA
plot(x_masked[,,,1])
```

```{r withMasking, fig.cap="Comparing computation time before and after masking of raster data. There is no performance increase by masking before aggregating."}
autoplot(
  microbenchmark(
No_masking =  x_aug %>%
  st_apply(1:2, mean) %>%
  setNames(paste0("v_",temp_names)),
Masking =  x_masked %>%
  st_apply(1:2, mean) %>%
  setNames(paste0("v_",temp_names))
 ), times=30
)
 
```

Since there is no increase in performance from masking, as we see in Fig. \@ref(fig: withMasking) then it basically means we it is slower to mask beforehand, since the masking was not past of the benchmark assessment.

So, this is my solution:

```{r}
# setNams dont work on stars proxy obejct, so I use rename instead
lookup <- setNames("mean", paste0("v_", temp_names))

x_summerMean <-   x_aug %>%
  st_apply(1:2, mean) %>%
  rename(all_of(lookup))
```

The *v\_* follows the naming convention in Norway.

```{r, fig.cap="Plotting the dummy data showing Attribute_A for the two dates as small maps, and a larger map showing the mean for year 2018"}
ggarrange(
ggplot() + 
  geom_stars(data = x_aug) +
  coord_equal() +
  facet_wrap(~time) +
  theme_void() +
  scale_fill_viridis_c(option = "D") +  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0)),  
  
  
ggplot() + 
  geom_stars(data = x_summerMean) +
  coord_equal() +
  #facet_wrap(~time) +
  theme_void() +
  scale_fill_viridis_c(option = "D") +
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0))
)
```

Here's the whole first step of the conceptual workflow in action, on the dummy data.

```{r}

# Setting up the parameters
dir <- substr(getwd(), 1,2)
path <- ifelse(dir == "C:",
       "P:/", 
       "/data/P-Prosjekter2/")
path2 <- paste0(path, 
                "41201785_okologisk_tilstand_2022_2023/data/climate_indicators/dummy_data")
start_month <-  "06"
end_month <- "08"
myFiles <- list.files(path2, pattern=".rds", full.names = T)
summerTemp_fullSeries <- NULL

# Looping though the files in the directory
system.time({
for(i in 1:length(myFiles)){
  
  temp <- readRDS(myFiles[i]) %>%
    select(Attribute_A)
  
  year_temp <- year(st_get_dimension_values(temp, "time")[1])
  start <- ym(paste(year_temp, start_month, sep="-"))
  end <-  ym(paste(year_temp, end_month, sep="-"))
  myInterval <- interval(start, end)
  lookup <- setNames("mean", paste0("v_", year_temp))

  temp <- temp %>%
    filter(time %within% myInterval) %>%
    st_apply(1:2, mean) %>%
    rename(all_of(lookup))
  
  summerTemp_fullSeries <- c(temp, summerTemp_fullSeries)
  rm(temp) # same computation time with and without this function, but more tidy this way
}


# Turn the attributes into a dimension and rename the new attribute
summerTemp_fullSeries <- summerTemp_fullSeries %>%
  merge(name="Year") %>%
  setNames("Attribute_A")
})
```

And this is the result.

```{r}
ggplot()+
  geom_stars(data = summerTemp_fullSeries) +
  coord_equal() +
  theme_void() +
  scale_fill_viridis_c(option = "D") +  
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0))+
  facet_wrap(~Year)
```

Now let's try it on the real data.

_I initially tried to read stars proxy object and add steps to the call_list and execute these with `st_as_stars`, but this still returned proxy objects. Therefore I read the whole file in without proxy. To save time I can subset based on Julian dates, but this is a hard coded solution._

```{r, warning=F}
path <- path <- ifelse(dir == "C:", 
      "R:/",
      "/data/R/")
  
path2 <- paste0(path, "GeoSpatialData/Meteorology/Norway_SeNorge2018_v22.09/Original/rr_tm/")

myFiles <- list.files(path2, pattern=".nc$",full.names = T)
real_temp_summer <- NULL

cl <- makeCluster(10L)

# Get julian days
temp <- stars::read_ncdf(paste(myFiles[1]))
start_month_num <-  6
end_month_num <- 8

julian_start <- yday(st_get_dimension_values(temp, "time")[1] %m+%
                       months(+start_month_num))
julian_end <- yday(st_get_dimension_values(temp, "time")[1] %m+%
                     months(+end_month_num))
step <- julian_end-julian_start



for(i in 1:length(myFiles)){
  
  tic("init")
  temp <- stars::read_ncdf(paste(myFiles[i]), var="tg", proxy=F,
                           ncsub = cbind(start = c(1, 1, julian_start), 
                              count = c(NA, NA, step)))
  year_temp <- year(st_get_dimension_values(temp, "time")[1])
  print(year_temp)
  #start <- ym(paste(year_temp, start_month, sep="-"))
  #end <-  ym(paste(year_temp, end_month, sep="-"))
  #myInterval <- interval(start, end)
  #myInterval <- interval(start, start+1)
  lookup <- setNames("mean", paste0("v_", year_temp))
  st_crs(temp) <- 25833
  toc()

  tic("filter and st_apply")
  temp <- temp %>%
    #filter(time %within% myInterval) %>%
    st_apply(1:2, mean, CLUSTER = cl) %>%
    rename(all_of(lookup)) 
  toc()
  
  tic("c()")
  real_temp_summer <- c(temp, real_temp_summer)
  #rm(temp)
  toc()
}

tic("Merge")
real_temp_summer <- real_temp_summer %>%
  merge(name = "Year") %>%
  setNames("climate_variable")
toc()

stopCluster(cl)

```

Here's a test for the effect of splitting over more cores.
```{r, eval=F}
cl <- makeCluster(2L)
cl2 <- makeCluster(6L)
cl3 <- makeCluster(10L)


tic("No cluster")
temp %>%
    #filter(time %within% myInterval) %>%
    st_apply(1:2, mean) %>%
    rename(all_of(lookup)) 
toc()

tic("Two clusters")
temp %>%
    #filter(time %within% myInterval) %>%
    st_apply(1:2, mean, CLUSTER = cl) %>%
    rename(all_of(lookup)) 
stopCluster(cl)
toc()

tic("Six clusters")
temp %>%
    #filter(time %within% myInterval) %>%
    st_apply(1:2, mean, CLUSTER = cl2) %>%
    rename(all_of(lookup)) 
stopCluster(cl2)
toc()

tic("Ten clusters")
temp %>%
    #filter(time %within% myInterval) %>%
    st_apply(1:2, mean, CLUSTER = cl3) %>%
    rename(all_of(lookup)) 
stopCluster(cl3)
toc()
  
```
No cluster: 21.401 sec elapsed
Two clusters: 21.546 sec elapsed
Six clusters: 14.548 sec elapsed
Ten clusters: 15.29 sec elapsed
Ten clusters (second run): 13.617 sec elapsed

The RStudio server has 48 cores. More parallel cores is probably on average faster. The NINA guidelines is to use max 10 cores, and to remember to close parallel cluster when done.

```{r}
#write_stars(real_temp_summer, "P:/41201785_okologisk_tilstand_2022_2023/data/climate_indicators/dummy_data/real_temp_summer.tif")
```

I might want to merge this

```{r}
real_temp_summer <- real_temp_summer %>%
  merge(name = "Year") %>%
  setNames("climate_variable")
```


```{r}
ggplot()+
  geom_stars(data = real_temp_summer[,,,1:2], downsample = c(10, 10, 0)) +
  coord_equal() +
  theme_void() +
  scale_fill_viridis_c(option = "D") +  
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0))+
  facet_wrap(~Year)
```


### 2. Calculate reference values
#### Aggregate across reference years

For this step we need some more dummy data from different years. I made data sets y1 to y5 above.

```{r}
# Function to add random noise
myRandom <- function(x) x*rnorm(1,1,.05)
```

```{r}
y1 <- x_summerMean
y2 <- x_summerMean %>% st_apply(1:2, myRandom) %>% setNames(paste0("v_",temp_names-1)) 
y3 <- x_summerMean %>% st_apply(1:2, myRandom) %>% setNames(paste0("v_",temp_names-2)) 
y4 <- x_summerMean %>% st_apply(1:2, myRandom) %>% setNames(paste0("v_",temp_names-3)) 
y5 <- x_summerMean %>% st_apply(1:2, myRandom) %>% setNames(paste0("v_",temp_names-4)) 
# We combine the data into one cube:
y <- c(y1, y2, y3, y4, y5) %>% 
  merge(name = "Year") %>%
  setNames("Attribute_A")
```

```{r}
ggplot() + 
  geom_stars(data = y) +
  coord_equal() +
  theme_void() +
  scale_fill_viridis_c(option = "D") +  
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0))+
  facet_wrap(~Year)
```

Now we aggregate across years, again using st_apply.

```{r}
median_sd <- function(x) { c(median = median(x), sd = sd(x))}
```

```{r}
y_ref <- y %>%
  st_apply(c("x", "y"), FUN =  median_sd)
```

```{r}
dim(y_ref)
```

```{r}
st_get_dimension_values(y_ref, "median_sd")
```

It's perhaps easier if median and sd are unique attributes, rather than levels of a dimension.

```{r}
y_ref <- y_ref %>% split("median_sd")
```

The attribute name *mean* we can change this to something more meaningful.

```{r}
y_ref <- setNames(y_ref, c("reference_upper", "sd"))
```

```{r}
tmap_arrange(
tm_shape(y_ref)+
  tm_raster("reference_upper")
,
tm_shape(y_ref)+
  tm_raster("sd",
            palette = "-viridis")
)
```

#### Export

I should export the reference data at this point (write to file) as we don't need to recalculate this every time we want to update the indicator values.

#### Calculate variable values

The climate variable is the same as y1, y2, y~*n*~, .... But to create some differences between the variable and the reference values, I will tweak these slightly, taking them down 2%. I then merge with the reference value and sd, creating one common data cube.

```{r}
y_var <- y %>% mutate(Attribute_A = Attribute_A * 0.98) %>% 
  split(3) %>% c(y_ref)
```

#### Normalise variable

```{r}
# select the columns to normalise
cols <- names(y_var)[!names(y_var) %in% c("reference_upper", "sd") ]
cols_new <- cols
names(cols_new) <- gsub("v_", "i_", cols)
```

```{r}
# Mutate
# can I find a way to keep the v_ columns?
system.time(
y_var_norm <- y_var %>%
  mutate(lower = reference_upper - 5*sd ) %>%
  mutate(threshold = reference_upper -2*sd ) %>%
  mutate(across(all_of(cols), ~ if_else(.x < threshold, 
                                        (.x - lower) / (threshold - lower),
                                        (.x - threshold) / (reference_upper - threshold),
                                        ))) %>%
  mutate(across(all_of(cols), ~ if_else(.x > 1, 1, .x))) %>%
  mutate(across(all_of(cols), ~ if_else(.x < 0, 0, .x))) %>%
  rename(all_of(cols_new)) %>%
  c(select(y_var, all_of(cols)))
)
```

I'm not sure mutate-across is the fastest way to do this. Perhaps st_apply would be better.

```{r}
ggarrange(
ggplot() + 
  geom_stars(data = y_var_norm["v_1970"]) +
  coord_equal() +
  theme_void() +
  scale_fill_viridis_c(option = "D") +  
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0))
,
ggplot() + 
  geom_stars(data = y_var_norm["reference_upper"]) +
  coord_equal() +
  theme_void() +
  scale_fill_viridis_c(option = "D") +  
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0))
,
ggplot() + 
  geom_stars(data = y_var_norm["i_1970"]) +
  coord_equal() +
  theme_void() +
  scale_fill_viridis_c(option = "D") +  
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0))
, ncol=3, align = "hv"
)
```

This looks like perfect noise, and it is, because the variable and reference value are so well correlated. If the reference value was static, or if there was a pattern in v_1970, the scaled indicator map would look different.

#### Plot time series

```{r}
  ts1 <- y_var_norm %>%
  select(all_of(starts_with("i_"))) %>%
  st_redimension() %>%
  st_apply(3, median_sd) %>%
  as_tibble() %>%
  setNames(c("metric", "i_year", "value") ) %>%
  mutate(year = as.numeric(substr(i_year, 3, 7))) %>%
  pivot_wider(names_from = "metric") %>%
  select(-i_year) %>%
  add_column(type = "indicator_value") %>%
  rename(value = median) %>%
  add_column(metric = "median")
```

```{r}
ts2 <- y_var_norm %>%
  select(all_of(starts_with("v_"))) %>%
  st_redimension() %>%
  st_apply(3, median_sd) %>%
  as_tibble() %>%
  setNames(c("metric", "v_year", "value") ) %>%
  mutate(year = as.numeric(substr(v_year, 3, 7))) %>%
  pivot_wider(names_from = "metric") %>%
  select(-v_year) %>%
  add_column(type = "variable_value") %>%
  rename(value = median) %>%
  add_column(metric = "median")
```

```{r}
  ts3 <- y_var_norm %>%
  select(!all_of(starts_with(c("v_", "i_")))) %>%
  st_redimension() %>%
  st_apply(3, median) %>%
  as_tibble() %>%
  setNames(c("type", "value")) %>%
    add_column(metric = "median")
```

```{r}
(ts <- bind_rows(ts1, ts2, ts3))
```

Calculate diference between indicator and reference We don't want to include this in ts, but as a temp file.

```{r}
tsX <- ts1
```

```{r}
ts %>% filter(type == "indicator_value") %>%
  ggplot(aes(x = year, y = value))+
  geom_line()
```

### Scaled indicator values

<!-- Text and analyses here -->

### Uncertainty

<!-- Text here -->

## Prepare export

<!-- Text here -->

### Eksport file (final product)

<!-- Export final file. Ideally a georeferenced shape or raster wit indicators values (raw and normalised), reference values and errors. -->
