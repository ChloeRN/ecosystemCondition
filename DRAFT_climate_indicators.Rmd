---
title: "Climate indicators"
output:
  html_document
---

# Cilmate indicators

<br />

*Author and date:* <!-- Write your name here -->

<br />

<!-- Load all you dependencies here -->

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stars)
library(sf)
library(tmap)
library(lubridate)
library(ggpubr)
dir <- substr(getwd(), 1,2)

```

```{r, echo=F}
Ecosystem <- "All" # e.g. "Skog og fjell"
Egenskap  <- "Abiotiske egenskaper" # e.g. "Primærproduksjon"
ECT       <- "Physical state characteristics" # e.g. "Structural state characteristic"
Contact   <- "Anders L. Kolstad" # e.g. "Anders Kolstad"
```

```{r, echo=F}
metaData <- data.frame(Ecosystem,
                       "Økologisk egenskap" = Egenskap,
                       "ECT class" = ECT)
knitr::kable(metaData)
```

<br /> <br />

<hr />

## Introduction

This chaters describes a workflow for generating or preparing indicators based on interpolated climate data from [SeNorge](https://senorge.no/).

## About the underlying data

The data is in a raster format and extends back to 1957 in the form of multiple interpolated climate variables. The spatial resolution is 1 x 1 km.

### Representativity in time and space

The data includes the last normal period (1961-1990) which defines the reference condition for climate variables. Therefore the temporal resolution is very good. Also considering tha daily resolution of the data.

Spatially, a 1x1 km resolution is sufficient for most climate variables, esp. in homogeneous terrain, but this needs to be evaluation for each variable and scenario specifically.

### Original units

Varied

### Temporal coverage

1957 - present

### Aditional comments about the dataset

The data format has changes from .BIL to .nc (netcdf) and now a single file contains all the rasters for one year (365 days), and sometimes for multiple variables also.

## Ecosystem characteristic

### Norwegain standard

These variables typically will fall under the *abiotiske egenskaper* class.

### SEEA EA

In SEEA EA, these variables will typically fall under A1 - Physical state characteristics.

## Collinearities with other indicators

Climate variables are most likely to be correlated with each other (e.g. temperature and snow). Also, some climate variables are better classed as pressure indicators, and these might have a causal association with several condition indicators.

## Reference condition and values

### Reference condition

The reference condition for climate variables is defined as the laste normal period 1961-1990.

### Reference values, thresholds for defining *good ecological condition*, minimum and/or maximum values

-   Un-scaled indicator value = median value over last 5 year (moving window)

-   Upper reference level (best possible condition) = median value from the reference period

-   Threshold for good ecosystem condition = 2 standard deviation units for the climate variable during the reference period.

-   Lower reference value = 5 standard deviation units for the climate variable during the reference period.

## Uncertainties

For the indicator map (1 x 1 km raster) there is no uncertainty associated with the indicator values. For aggregated indicator values (e.g. for regions), the uncertainty in the indicator value is calculated from the spatial variation in the indicator values via bootstrapping.

## References

<https://senorge.no/>

rr and tm are being download from <https://thredds.met.no/thredds/catalog/senorge/seNorge_2018/Archive/catalog.html>

### Additional resources 

[Stars package](https://r-spatial.github.io/stars/)

[R as a GIS for economists](https://tmieno2.github.io/R-as-GIS-for-Economists/stars-basics.html) chapter 7

## Analyses

### Data set

The data is downloaded to a local NINA server, and updated regularly.

```{r}
path <- ifelse(dir == "C:", 
      "R:/GeoSpatialData/Meteorology/Norway_SeNorge2018_v22.09/Original",
      "/data/R/GeoSpatialData/Meteorology/Norway_SeNorge2018_v22.09/Original")
```

This folder contains folder for the different parameters

```{r}
(files <- list.files(path))
```

This table explains them in more detail

```{r}
senorgelayers <- read_delim("data/senorgelayers.txt", 
    delim = "\t", escape_double = FALSE, 
    trim_ws = TRUE)
DT::datatable(senorgelayers)
```

Here is the content of one of these folders (first 10 entries)

```{r}
(files_tm <- list.files(paste0(path, "/rr_tm")))[1:10]
```

There are `r length(files_tm)` files, i.e. `r length(files_tm)` years of data, one file per year.

Importing (proxy) file:

```{r, warning=F}
tg_1957 <- stars::read_ncdf(paste0(path, "/rr_tm/seNorge2018_1957.nc"))
```

I think I know the CRS, so setting it manually.

```{r}
st_crs(tg_1957) <- 25833
```

The data has three dimension

```{r}
dim(tg_1957)
```

... and four attributes

```{r}
names(tg_1957)
```

Precipitation (rr), mean temperature (tg), max temperatur (tx) and min temperature (tn) are included in the same file.

For this example workflow I will only focus on tg (temperature). I could subset the data like this

```{r}
tg_1957 %>% select(tg)
```

, but since this is a proxy object, it will not initiate any change untill later, when I write to file and all the lazy operations are done in squeal. Therefore, I will import a test data set, which is smaller and which I can import to memory. Then I can perform the operations on that data set and we can see the results.

### Dummy data

This test data is included in the {stars} package

```{r}
tif = system.file("tif/L7_ETMs.tif", package = "stars")
t1 = as.Date("2018-05-31")
x = read_stars(c(tif, tif, tif, tif), along = 
                  list(time = c(t1, t1+1, t1+50, t1+100)), 
               RasterIO = list(nXOff = c(1), 
                               nYOff = c(1), 
                               nXSize = 50, 
                               nYSize = 50, 
                               bands = c(1:6)))
```

A single attribute

```{r}
names(x)
```

I can rename it like this
```{r}
x <- setNames(x, "Attribute_A")
```


And I can add another dummy attribute.

```{r}
x <- x %>%
  mutate("Attribute_B" = Attribute_A/2)
```


The dummy data also has four dimensions

```{r}
dim(x)
```

X and y area the coordinates. Band is an integer:

```{r}
st_get_dimension_values(x, "band")
```


And time is four dates covering four months in 2018:

```{r}
st_get_dimension_values(x, "time")
```

#### Regions

Importing a shape file with the regional delineation.

```{r}
reg <- sf::st_read("data/regions.shp")
#st_crs(reg)
```

#### Ecosystem map

Coming soon ....


### Workflow

The general workflow is like this:

1.  Calculate reference values

    -   Import correct .nc file (loop though year 1961-1990) and subset to the correct attribute (above)

    -   Filter data by dates (optional)

    -   Aggregate across time within a year

    -   Aggregate across years to get median and sd values

    -   Intersect with accounting area polygons

2.  Calculate variable values

    -   Import correct .nc file (loop though the years 1961 to present) and subset to the correct attribute (above)

    -   Repeat the steps from when calculating the reference value, except for calculating the sd.

    -   Add layers to data cube already containing reference values

3.  Normalize climate variable at the individual grid cell level using the three reference values

4.  Write to file (for use with other ecosystem types)

5.  Mask by ecosystem type

6.  Aggregate in space (to accounting areas)

    -   Aggregate across 5 year time steps to smooth out random interannual variation and leav climate signal

    -   Intersect with accounting area polygons

    -   Bootstrap indicator values (cell values) and get median and sd

7.  Make trend figure


I will try to use [parallelization](https://tmieno2.github.io/R-as-GIS-for-Economists/EE.html) to speed things up. Also, masking to ecosystem time earlier in the workflow might prove smart.  



### Calculate reference values
#### Import

I need to code a for loop here for importing the .nc files.

But for then selecting the attribute, simply do like this:

```{r}
names(x)
```
```{r}
x_sub <- x %>% 
  select(Attribute_A)
names(x_sub)
```

#### Filter data by dates

Say we want to calculate the mean summer temperature. We then want to exclude the data for the times that is not withing our definition of summer.

Let's try it on the dummy data. Remember this data had three time steps.
```{r}
st_get_dimension_values(x, "time")
```

Our data has 365 time steps for each year.

First I define my start and end dates. I want to keep only the summer data, defined as jun - aug.

```{r}
start <-  ymd("2018-06-01")
end <-  ymd("2018-08-31")
summer <- interval(start, end)
```

Then I filter

```{r}
x_aug <- x_sub %>%
  filter(time %within% summer)
st_get_dimension_values(x_aug, "time")
```

Let me introduce some differences to Attribute_A between these two dates.
```{r}
x_aug$Attribute_A[,,,2] <- x_aug$Attribute_A[,,,1]*4
```


#### Aggregate across time within a year

For this example I want to calculate the mean temperature over the summer.
I therefore need to aggregate over time. Here is how:

```{r}
x_summerMean <- x_aug %>%
  filter(band ==1) %>% # for our real data we don't need this filter
  st_apply(1:2, mean) 

dim(x_summerMean)
```

The time (and band) dimension is now gone as we have aggregated accoss it.

We can also use the aggregate function to produce a data cube with lower temporal resolution, for example per year:

```{r}
x_summerMean_v2 <- x_aug %>%
  aggregate(by = "year", mean)
```

Actually, this second method is better because we don't ned to filter out additional dimensions, and it retains the attribute name (the first method replaced it with the function name _mean_).

```{r, fig.cap="Plotting the dummy data showing Attribute_A for the two dates as small maps, and a larger map showing the mean for year 2018"}
ggarrange(
ggplot() + 
  geom_stars(data = x_aug) +
  coord_equal() +
  facet_wrap(~time) +
  theme_void() +
  scale_fill_viridis_c(option = "D") +  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0)),  
  
  
ggplot() + 
  geom_stars(data = x_summerMean_v2) +
  coord_equal() +
  facet_wrap(~time) +
  theme_void() +
  scale_fill_viridis_c(option = "D") +
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0))
)
```





### Scaled indicator values

<!-- Text and analyses here -->

### Uncertainty

<!-- Text here -->

## Prepare export

<!-- Text here -->

### Eksport file (final product)

<!-- Export final file. Ideally a georeferenced shape or raster wit indicators values (raw and normalised), reference values and errors. -->
