---
title: "Climate indicators"
output:
  html_document
---

# Cilmate indicators

<br />

*Author and date:* <!-- Write your name here -->

<br />

<!-- Load all you dependencies here -->

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stars)
library(sf)
library(tmap)
library(lubridate)
library(ggpubr)
library(microbenchmark)
dir <- substr(getwd(), 1,2)

```

```{r, echo=F}
Ecosystem <- "All" # e.g. "Skog og fjell"
Egenskap  <- "Abiotiske egenskaper" # e.g. "Primærproduksjon"
ECT       <- "Physical state characteristics" # e.g. "Structural state characteristic"
Contact   <- "Anders L. Kolstad" # e.g. "Anders Kolstad"
```

```{r, echo=F}
metaData <- data.frame(Ecosystem,
                       "Økologisk egenskap" = Egenskap,
                       "ECT class" = ECT)
knitr::kable(metaData)
```

<br /> <br />

<hr />

## Introduction

This chaters describes a workflow for generating or preparing indicators based on interpolated climate data from [SeNorge](https://senorge.no/).

## About the underlying data

The data is in a raster format and extends back to 1957 in the form of multiple interpolated climate variables. The spatial resolution is 1 x 1 km.

### Representativity in time and space

The data includes the last normal period (1961-1990) which defines the reference condition for climate variables. Therefore the temporal resolution is very good. Also considering tha daily resolution of the data.

Spatially, a 1x1 km resolution is sufficient for most climate variables, esp. in homogeneous terrain, but this needs to be evaluation for each variable and scenario specifically.

### Original units

Varied

### Temporal coverage

1957 - present

### Aditional comments about the dataset

The data format has changes from .BIL to .nc (netcdf) and now a single file contains all the rasters for one year (365 days), and sometimes for multiple variables also.

## Ecosystem characteristic

### Norwegain standard

These variables typically will fall under the *abiotiske egenskaper* class.

### SEEA EA

In SEEA EA, these variables will typically fall under A1 - Physical state characteristics.

## Collinearities with other indicators

Climate variables are most likely to be correlated with each other (e.g. temperature and snow). Also, some climate variables are better classed as pressure indicators, and these might have a causal association with several condition indicators.

## Reference condition and values

### Reference condition

The reference condition for climate variables is defined as the laste normal period 1961-1990.

### Reference values, thresholds for defining *good ecological condition*, minimum and/or maximum values

-   Un-scaled indicator value = median value over last 5 year (moving window)

-   Upper reference level (best possible condition) = median value from the reference period

-   Threshold for good ecosystem condition = 2 standard deviation units for the climate variable during the reference period.

-   Lower reference value = 5 standard deviation units for the climate variable during the reference period.

## Uncertainties

For the indicator map (1 x 1 km raster) there is no uncertainty associated with the indicator values. For aggregated indicator values (e.g. for regions), the uncertainty in the indicator value is calculated from the spatial variation in the indicator values via bootstrapping.

## References

<https://senorge.no/>

rr and tm are being download from <https://thredds.met.no/thredds/catalog/senorge/seNorge_2018/Archive/catalog.html>

### Additional resources

[Stars package](https://r-spatial.github.io/stars/)

[R as a GIS for economists](https://tmieno2.github.io/R-as-GIS-for-Economists/stars-basics.html) chapter 7

## Analyses

### Data set

The data is downloaded to a local NINA server, and updated regularly.

```{r}
path <- ifelse(dir == "C:", 
      "R:/GeoSpatialData/Meteorology/Norway_SeNorge2018_v22.09/Original",
      "/data/R/GeoSpatialData/Meteorology/Norway_SeNorge2018_v22.09/Original")
```

This folder contains folder for the different parameters

```{r}
(files <- list.files(path))
```

This table explains them in more detail

```{r}
senorgelayers <- read_delim("data/senorgelayers.txt", 
    delim = "\t", escape_double = FALSE, 
    trim_ws = TRUE)
DT::datatable(senorgelayers)
```

Here is the content of one of these folders (first 10 entries)

```{r}
(files_tm <- list.files(paste0(path, "/rr_tm")))[1:10]
```

There are `r length(files_tm)` files, i.e. `r length(files_tm)` years of data, one file per year.

Importing (proxy) file:

```{r, warning=F}
tg_1957 <- stars::read_ncdf(paste0(path, "/rr_tm/seNorge2018_1957.nc"))
```

I think I know the CRS, so setting it manually.

```{r}
st_crs(tg_1957) <- 25833
```

The data has three dimension

```{r}
dim(tg_1957)
```

... and four attributes

```{r}
names(tg_1957)
```

Precipitation (rr), mean temperature (tg), max temperatur (tx) and min temperature (tn) are included in the same file.

For this example workflow I will only focus on tg (temperature). I could subset the data like this

```{r}
tg_1957 %>% select(tg)
```

, but since this is a proxy object, it will not initiate any change untill later, when I write to file and all the lazy operations are done in squeal. Therefore, I will import a test data set, which is smaller and which I can import to memory. Then I can perform the operations on that data set and we can see the results.

### Dummy data

This test data is included in the {stars} package

```{r}
tif = system.file("tif/L7_ETMs.tif", package = "stars")
t1 = as.Date("1970-05-31")
x = read_stars(c(tif, tif, tif, tif), along = 
                  list(time = c(t1, t1+1, t1+50, t1+100)), 
               RasterIO = list(nXOff = c(1), 
                               nYOff = c(1), 
                               nXSize = 50, 
                               nYSize = 50, 
                               bands = c(1:6)))
```

A single attribute

```{r}
names(x)
```

I can rename it like this

```{r}
x <- setNames(x, "Attribute_A")
```

And I can add another dummy attribute.

```{r}
x <- x %>%
  mutate("Attribute_B" = Attribute_A/2)
```

The dummy data also has four dimensions

```{r}
dim(x)
```

X and y area the coordinates. Band is an integer:

```{r}
st_get_dimension_values(x, "band")
```

And time is four dates covering four months in 1970:

```{r}
st_get_dimension_values(x, "time")
```

#### Regions

Importing a shape file with the regional delineation.

```{r}
reg <- sf::st_read("data/regions.shp")
#st_crs(reg)
```

#### Ecosystem map

Coming soon ....

### Workflow

The general workflow is like this:

1.  Calculate reference values

    -   Import correct .nc file (loop though year 1961-1990) and subset to the correct attribute (above)

    -   Filter data by dates (optional)

    -   Aggregate across time within a year

    -   Aggregate across years to get median and sd values

    -   Intersect with accounting area polygons

2.  Calculate variable values

    -   Import correct .nc file (loop though the years 1961 to present) and subset to the correct attribute (above)

    -   Repeat the steps from when calculating the reference value, except for calculating the sd.

    -   Add layers to data cube already containing reference values

3.  Normalize climate variable at the individual grid cell level using the three reference values

4.  Write to file (for use with other ecosystem types)

5.  Mask by ecosystem type

6.  Aggregate in space (to accounting areas)

    -   Aggregate across 5 year time steps to smooth out random interannual variation and leav climate signal

    -   Intersect with accounting area polygons

    -   Bootstrap indicator values (cell values) and get median and sd

7.  Make trend figure

I will try to use [parallelization](https://tmieno2.github.io/R-as-GIS-for-Economists/EE.html) to speed things up. Also, masking to ecosystem time earlier in the workflow might prove smart.

### Calculate reference values

#### Import

I need to code a for loop here for importing the .nc files.

But for then selecting the attribute, simply do like this:

```{r}
names(x)
```

```{r}
x_sub <- x %>% 
  select(Attribute_A)
names(x_sub)
```

#### Filter data by dates

Say we want to calculate the mean summer temperature. We then want to exclude the data for the times that is not withing our definition of summer.

Let's try it on the dummy data. Remember this data had three time steps.

```{r}
st_get_dimension_values(x, "time")
```

Our data has 365 time steps for each year.

First I define my start and end dates. I want to keep only the summer data, defined as jun - aug.

```{r}
start_month <-  "06"
end_month <- "08"
year_temp <- year(st_get_dimension_values(x, "time")[1])
start <- ym(paste(year_temp, start_month, sep="-"))
end <-  ym(paste(year_temp, end_month, sep="-"))
summer <- interval(start, end)
```

Then I filter

```{r}
x_aug <- x_sub %>%
  filter(time %within% summer)
st_get_dimension_values(x_aug, "time")
```

Let me introduce some differences to Attribute_A between these two dates.

```{r}
x_aug$Attribute_A[,,,2] <- x_aug$Attribute_A[,,,1]*4
```

#### Aggregate across time within a year

For this example I want to calculate the mean temperature over the summer. I therefore need to aggregate over time. There are two ways.

*Method 1:*

```{r}
microOut <- microbenchmark(
st_apply =  x_aug %>%
  filter(band ==1) %>% # for our real data we don't need this filter
  st_apply(1:2, mean, rename=F),
aggregate = x_aug %>%
  aggregate(by = "year", mean),
times=30
)
```

The time (and band) dimension is now gone as we have aggregated across it.

```{r, fig.cap="Comparing computation tome for two spatial aggregation functions."}
autoplot(microOut)
```

`st_apply` is considerably faster, even wit the filtering step, but `aggregate` has the advantage of that it retains the time dimension. 
I think I will try to use `aggregate`, and it it takes too long on the real data, then I can come back to this point, and perhaps switch to st_apply. 


```{r}
x_summerMean <-   x_aug %>%
  filter(band ==1) %>% 
  st_apply(1:2, mean, rename=F)
```



```{r, fig.cap="Plotting the dummy data showing Attribute_A for the two dates as small maps, and a larger map showing the mean for year 2018"}
ggarrange(
ggplot() + 
  geom_stars(data = x_aug) +
  coord_equal() +
  facet_wrap(~time) +
  theme_void() +
  scale_fill_viridis_c(option = "D") +  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0)),  
  
  
ggplot() + 
  geom_stars(data = x_summerMean_v2) +
  coord_equal() +
  facet_wrap(~time) +
  theme_void() +
  scale_fill_viridis_c(option = "D") +
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0))
)
```

#### Aggregate across years

For this step we need some more dummy data from different years.

First, let me cut the _band_ dimension.
```{r}
temp <- x_summerMean_v2 %>%
  filter(band==1) %>%
  adrop(drop=4) %>%
  aperm(c(2,3,1))
dim(temp)
```


```{r}
oldTime <- st_get_dimension_values(x_summerMean_v2, "time")

# Function to add random noise
myRandom <- function(x) x*rnorm(1,1,.05)

y1 <- temp
y2 <- temp %>% st_set_dimensions("time", values = oldTime %m+% years(-1)) %>% st_apply(c("x", "y", "time"), FUN = myRandom)
y3 <- temp %>% st_set_dimensions("time", values = oldTime %m+% years(-2)) %>% st_apply(c("x", "y", "time"), FUN = myRandom)
y4 <- temp %>% st_set_dimensions("time", values = oldTime %m+% years(-3)) %>% st_apply(c("x", "y", "time"), FUN = myRandom)
y5 <- temp %>% st_set_dimensions("time", values = oldTime %m+% years(-4)) %>% st_apply(c("x", "y", "time"), FUN = myRandom)


# We combine the data into one cube:
y <- c(y1, y2, y3, y4, y5)
```

```{r}
ggplot() + 
  geom_stars(data = y) +
  coord_equal() +
  theme_void() +
  scale_fill_viridis_c(option = "D") +  
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0))+
  facet_wrap(~time)
```

Now we aggregate across years. For the reference period is is probably best to use *Method 1*, and for he indicator variable (the moving window estimation) probably *Method 2* is best. So Method 1 now.

```{r}
median_sd <- function(x) { c(median = median(x), sd = sd(x))}
```

```{r}
y_ref <- y %>%
  st_apply(c("x", "y"), FUN =  median_sd)
```

```{r}
dim(y_ref)
```

```{r}
st_get_dimension_values(y_ref, "median_sd")
```

It's perhaps easier if median and sd are unique attributes, rather than levels of a dimension.

```{r}
y_ref <- y_ref %>% split("median_sd")
```

The attribute name *mean* we can change this to something more meaningful.

```{r}
y_ref <- setNames(y_ref, c("reference_value", "sd"))
```

```{r}
tmap_arrange(
tm_shape(y_ref)+
  tm_raster("reference_value")
,
tm_shape(y_ref)+
  tm_raster("sd",
            palette = "-viridis")
)
```

#### Export
I should export the reference data at this point (write to file) as we don't need to recalculate this every time we want to update the indicator values.


#### Calculate variable values

The climate variable is the same as y1, y2, y~*n*~, ....
But to create some differences between the variable and the reference values, I will tweak these slightly, taking them down 2%. I then merge with the reference value and sd, creating one common data cube. 

```{r}
y_var <- y %>% mutate(Attribute_A = Attribute_A * 0.98) %>% 
  split(3) %>% c(y_ref)
y_var <- y_var %>%
  setNames(paste0("v_",substr(names(y_var), 1,4)))%>%
  rename("reference" = v_refe,
         "sd" = v_sd)
```

```{r}
# select the columns to normalise
cols <- names(y_var)[!names(y_var) %in% c("reference", "sd") ]
cols_new <- cols
names(cols_new) <- gsub("v_", "i_", cols)

# Mutate
# can I find a way to keep the v_ columns?
y_var_norm <- y_var %>%
  mutate(lower = reference - 5*sd ) %>%
  mutate(threshold = reference -2*sd ) %>%
  mutate(across(all_of(cols), ~ if_else(.x < threshold, 
                                        (.x - lower) / (threshold - lower),
                                        (.x - threshold) / (reference - threshold),
                                        ))) %>%
  mutate(across(all_of(cols), ~ if_else(.x > 1, 1, .x))) %>%
  mutate(across(all_of(cols), ~ if_else(.x < 0, 0, .x))) %>%
  rename(all_of(cols_new))
```


```{r}
ggarrange(
ggplot() + 
  geom_stars(data = y_var["v_1970"]) +
  coord_equal() +
  theme_void() +
  scale_fill_viridis_c(option = "D") +  
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0))
,
ggplot() + 
  geom_stars(data = y_var_norm["reference"]) +
  coord_equal() +
  theme_void() +
  scale_fill_viridis_c(option = "D") +  
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0))
,
ggplot() + 
  geom_stars(data = y_var_norm["i_1970"]) +
  coord_equal() +
  theme_void() +
  scale_fill_viridis_c(option = "D") +  
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0))
, ncol=3
)
```


#### Normalise variable 
I now want to do cell-by-cell computations across the _attributes_ dimension, and divide _Attribute_A_ by _reference_value_.

```{r}
# A function for normalization
# I need to add a break point and lower reference value here as well.
# And I need to add truncation.
myFUN <- function(a,b) (a/b)
```


```{r}
system.time(
  temp <- st_apply(y_v_merged, 
                   c("x", "y"),
                   FUN = myFUN
  )
)

```
This operation is quite fast!

```{r, fig.cap = "The normalised indicator for year 1970."}
tm_shape(temp)+
  tm_raster()+
  tm_layout(legend.outside = T)
```
This looks like perfect noise, and it is, because the variable and reference value are so well correlated.
If the reference value was static, it would look different.

```{r}
y_v_merged$value[,,2] <- 200
```

```{r}
ggplot() + 
  geom_stars(data = y_v_merged) +
  coord_equal() +
  theme_void() +
  scale_fill_viridis_c(option = "D") +  
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0))+
  facet_wrap(~attributes)
```

```{r}
temp <- st_apply(y_v_merged, 
                   c("x", "y"), 
                   #1:2,
                   FUN = myFUN
)
```

```{r, fig.cap = "The normalised indicator for year 1970 when the reference value is the same all over."}
tm_shape(temp)+
  tm_raster()+
  tm_layout(legend.outside = T)
```




### Scaled indicator values

<!-- Text and analyses here -->

### Uncertainty

<!-- Text here -->

## Prepare export

<!-- Text here -->

### Eksport file (final product)

<!-- Export final file. Ideally a georeferenced shape or raster wit indicators values (raw and normalised), reference values and errors. -->
